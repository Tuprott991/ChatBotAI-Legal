"""
File ch√≠nh ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng Chatbot AI
Ch·ª©c nƒÉng: 
- T·∫°o giao di·ªán web v·ªõi Streamlit
- X·ª≠ l√Ω t∆∞∆°ng t√°c chat v·ªõi ng∆∞·ªùi d√πng
- K·∫øt n·ªëi v·ªõi AI model ƒë·ªÉ tr·∫£ l·ªùi
"""

# === IMPORT C√ÅC TH∆Ø VI·ªÜN C·∫¶N THI·∫æT ===
import streamlit as st  # Th∆∞ vi·ªán t·∫°o giao di·ªán web
from dotenv import load_dotenv  # ƒê·ªçc file .env ch·ª©a API key
from database_module import seed_milvus, seed_milvus_live  # H√†m x·ª≠ l√Ω d·ªØ li·ªáu
from agent import get_retriever as get_openai_retriever, get_llm_and_agent as get_openai_agent
from ollama_agent import get_retriever as get_ollama_retriever, get_llm_and_agent as get_ollama_agent
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

from my_speech import transcribe_audio
from audio_recorder_streamlit import audio_recorder
import random
import base64

def get_base64(bin_file):
    with open(bin_file, 'rb') as f:
        data = f.read()
    return base64.b64encode(data).decode()


# def set_background(png_file):
#     bin_str = get_base64(png_file)
#     page_bg_img = '''
#     <style>
#     .sidebar {
#     background-image: url("data:image/png;base64,%s");
#     background-size: cover;
#     opacity: 1;
#     }
#     </style>
#     ''' % bin_str
#     st.markdown(page_bg_img, unsafe_allow_html=True)

# === THI·∫æT L·∫¨P GIAO DI·ªÜN TRANG WEB ===
def setup_page():
    """
    C·∫•u h√¨nh trang web c∆° b·∫£n
    """
    st.set_page_config(
        page_title="VinhHien ChatBot",  # Ti√™u ƒë·ªÅ tab tr√¨nh duy·ªát
        page_icon="ü§ñ",  # Icon tab``
        layout="wide"  # Giao di·ªán r·ªông
    )
    # set_background('thcsvinhhien.jpg')  # Thi·∫øt l·∫≠p h√¨nh n·ªÅn

# === KH·ªûI T·∫†O ·ª®NG D·ª§NG ===
def initialize_app():
    """
    Kh·ªüi t·∫°o c√°c c√†i ƒë·∫∑t c·∫ßn thi·∫øt:
    - ƒê·ªçc file .env ch·ª©a API key
    - C·∫•u h√¨nh trang web
    """
    load_dotenv()  # ƒê·ªçc API key t·ª´ file .env
    setup_page()  # Thi·∫øt l·∫≠p giao di·ªán

# === THANH C√îNG C·ª§ B√äN TR√ÅI ===
def setup_sidebar():
    """
    T·∫°o thanh c√¥ng c·ª• b√™n tr√°i v·ªõi c√°c t√πy ch·ªçn
    """
    with st.sidebar:
        st.title("‚öôÔ∏è C·∫•u h√¨nh")
        

        # Ph·∫ßn 1: Ch·ªçn Model ƒë·ªÉ tr·∫£ l·ªùi
        st.header("ü§ñ Model AI")
        model_choice = st.radio(
            "Ch·ªçn AI Model ƒë·ªÉ tr·∫£ l·ªùi:",
            ["OpenAI GPT-4", "xAI Grok (free API)", "Meta LLama3.1 (Ollama - Local)"]
        )

        # Ph·∫ßn 2: Ch·ªçn lƒ©nh v·ª±c 

        # Th√™m ph·∫ßn ch·ªçn collection ƒë·ªÉ query
        st.header("üîç Ch·ªçn lƒ©nh v·ª±c ph√°p lu·∫≠t")
        collections = ["Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô", "Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô (BGE-M3)", "Lu·∫≠t lao ƒë·ªông", "Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh", "Lu·∫≠t ƒë·∫•t ƒëai"]

        collection_to_query = st.selectbox(
            "Ch·ªçn lƒ©nh v·ª±c ph√°p lu·∫≠t ƒë·ªÉ truy v·∫•n:",
            collections,
            help="Ch·ªçn lƒ©nh v·ª±c ph√°p lu·∫≠t b·∫°n mu·ªën s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin"
        )

        if collection_to_query == "Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô":
            collection_to_query = "LGTDB"
        elif collection_to_query == "Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô (BGE-M3)":
            collection_to_query = "LGTDB_BGE"

        # Ph·∫ßn 3: Ch·ªçn Embeddings Model
        st.header("üî§ Embeddings Model")
        embeddings_choice = st.radio(
            "Ch·ªçn Embeddings Model:",
            ["OpenAI", "Ollama"]
        )
        use_ollama_embeddings = (embeddings_choice == "Ollama")
        
        # Ph·∫ßn 4: C·∫•u h√¨nh Data
        st.header("üìö N·∫°p th√™m d·ªØ li·ªáu")
        data_source = st.radio(
            "Ch·ªçn ngu·ªìn d·ªØ li·ªáu:",
            ["File Local", "URL tr·ª±c ti·∫øp"]
            
        )
        # X·ª≠ l√Ω ngu·ªìn d·ªØ li·ªáu d·ª±a tr√™n embeddings ƒë√£ ch·ªçn
        if data_source == "File Local":
            handle_local_file(use_ollama_embeddings)
        else:
            handle_url_input(use_ollama_embeddings)
            
        return model_choice, collection_to_query

def handle_local_file(use_ollama_embeddings: bool):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng ch·ªçn t·∫£i file
    """
    collection_name = st.text_input(
        "T√™n collection trong Milvus:", 
        "data_test",
        help="Nh·∫≠p t√™n collection b·∫°n mu·ªën l∆∞u trong Milvus"
    )
    filename = st.text_input("T√™n file JSON:", "stack.json")
    directory = st.text_input("Th∆∞ m·ª•c ch·ª©a file:", "data")
    
    if st.button("T·∫£i d·ªØ li·ªáu t·ª´ file"):
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu..."):
            try:
                seed_milvus(
                    'http://localhost:19530', 
                    collection_name, 
                    filename, 
                    directory, 
                    use_ollama=use_ollama_embeddings
                )
                st.success(f"ƒê√£ t·∫£i d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}")

def handle_url_input(use_ollama_embeddings: bool):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng ch·ªçn crawl URL
    """
    collection_name = st.text_input(
        "T√™n collection trong Milvus:", 
        "data_test_live",
        help="Nh·∫≠p t√™n collection b·∫°n mu·ªën l∆∞u trong Milvus"
    )
    url = st.text_input("Nh·∫≠p URL:", "https://www.stack-ai.com/docs")
    
    if st.button("Crawl d·ªØ li·ªáu"):
        if not collection_name:
            st.error("Vui l√≤ng nh·∫≠p t√™n collection!")
            return
            
        with st.spinner("ƒêang crawl d·ªØ li·ªáu..."):
            try:
                seed_milvus_live(
                    url, 
                    'http://localhost:19530', 
                    collection_name, 
                    'stack-ai', 
                    use_ollama=use_ollama_embeddings
                )
                st.success(f"ƒê√£ crawl d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")
            except Exception as e:
                st.error(f"L·ªói khi crawl d·ªØ li·ªáu: {str(e)}")


# === GIAO DI·ªÜN CHAT CH√çNH ===
def setup_chat_interface(model_choice):
    st.title("ü§ñüí¨ Legal Assistant AI ChatBot")
    
    # Caption ƒë·ªông theo model
    if model_choice == "OpenAI GPT-4":
        st.caption("‚ú® Tr·ª£ l√Ω AI ph√°p lu·∫≠t ƒë∆∞·ª£c th·ª±c hi·ªán b·ªüi nh√≥m h·ªçc sinh tr∆∞·ªùng THCS Vinh Hi·ªÅn h·ªó tr·ª£ b·ªüi framework LangChain")
    elif model_choice == "xAI Grok (free API)":
        st.caption("‚ú® Tr·ª£ l√Ω AI ph√°p lu·∫≠t ƒë∆∞·ª£c th·ª±c hi·ªán b·ªüi nh√≥m h·ªçc sinh tr∆∞·ªùng THCS Vinh Hi·ªÅn h·ªó tr·ª£ b·ªüi framework LangChain")
    else:
        st.caption("‚ú® Tr·ª£ l√Ω AI ƒë∆∞·ª£c th·ª±c hi·ªán b·ªüi nh√≥m h·ªçc sinh tr∆∞·ªùng THCS Vinh Hi·ªÅn h·ªó tr·ª£ b·ªüi framework LangChain v√† Open Source LLM Ollama LLaMA3.1")
    
    # Kh·ªüi t·∫°o b·ªô nh·ªõ chat
    msgs = StreamlitChatMessageHistory(key="langchain_messages") # ƒê√∫ng th√¨ ch·ªó n√†y n√™n l∆∞u chat history v√†o trong backend database
    
    # T·∫°o ƒëo·∫°n chat m·ªõi khi b·∫Øt ƒë·∫ßu 
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ch√†o b·∫°n, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"}
        ]
        msgs.add_ai_message("T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?")

    # hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs


# === X·ª¨ L√ù TIN NH·∫ÆN NG∆Ø·ªúI D√ôNG ===
def handle_user_input(msgs, agent_executor):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng g·ª≠i tin nh·∫Øn:
    1. Hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
    2. G·ªçi AI x·ª≠ l√Ω v√† tr·∫£ l·ªùi
    3. L∆∞u v√†o l·ªãch s·ª≠ chat
    """
    if prompt := st.chat_input("H√£y h·ªèi t√¥i ƒëi n√†o!!!" ):
        # L∆∞u v√† hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng

        st.session_state.messages.append({"role": "human", "content": prompt}) #session_state l√† qu·∫£n l√Ω cu·ªôc h·ªôi tho·∫°i ƒë√≥
        st.chat_message("human").write(prompt)
        msgs.add_user_message(prompt)

        # X·ª≠ l√Ω v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        with st.chat_message("assistant"):
            st_callback = StreamlitCallbackHandler(st.container())
            
            # L·∫•y l·ªãch s·ª≠ chat
            chat_history = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]
            ]

            # G·ªçi AI x·ª≠ l√Ω
            response = agent_executor.invoke(
                {
                    "input": prompt,
                    "chat_history": chat_history
                },
                {"callbacks": [st_callback]} # AI gen ra ch·ªØ n√†o th√¨ ch·ªó n√†y s·∫Ω h·ª©ng l·∫°i v√† tr·∫£ cho user 
            )

            # L∆∞u v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
            output = response["output"] # L·∫•y output
            st.session_state.messages.append({"role": "assistant", "content": output}) # Th√™m v√†o trong session state
            msgs.add_ai_message(output) # L∆∞u v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i
            st.write(output) # Ghi l√™n m√†n h√¨nh


def handle_user_input_with_microphone(msgs, agent_executor):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng g·ª≠i tin nh·∫Øn qua thanh chat ho·∫∑c s·ª≠ d·ª•ng Microphone:
    Thanh chat input v√† n√∫t micro n·∫±m li·ªÅn k·ªÅ nhau, kh√¥ng c·∫ßn chia c·ªôt.
    """
    # Chat input: cho ph√©p g√µ text 


    user_input = st.chat_input(placeholder="H√£y h·ªèi t√¥i b·∫±ng c√°ch nh·∫≠p ho·∫∑c nh·∫•n üé§ ƒë·ªÉ n√≥i...")

    audio_bytes = audio_recorder(
        text="", 
        recording_color="#e8b62c",
        neutral_color="#6aa36f",
        icon_name="microphone",
        icon_size="2x"
    )

    # X·ª≠ l√Ω ƒë·∫ßu v√†o t·ª´ microphone
    if audio_bytes:
        with st.spinner("üéôÔ∏è ƒêang nh·∫≠n di·ªán gi·ªçng n√≥i..."):
            # Ch·∫°y nh·∫≠n di·ªán √¢m thanh qua microphone
            text_from_audio = transcribe_audio(audio_bytes)
            if text_from_audio.startswith("L·ªói"):
                # Th√¥ng b√°o l·ªói n·∫øu kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c
                # st.error(text_from_audio)
                pass
            else:
                # Khi nh·∫≠n di·ªán th√†nh c√¥ng -> ƒëi·ªÅn s·∫µn v√†o input box
                # st.success(f"üì• Nh·∫≠n di·ªán: {text_from_audio}")
                st.session_state.user_temp_input = text_from_audio

    # N·∫øu c√≥ ƒë·∫ßu v√†o (t·ª´ thanh chat ho·∫∑c t·ª´ gi·ªçng n√≥i)
    if user_input or st.session_state.get("user_temp_input"):
        # L·∫•y prompt t·ª´ thanh chat ho·∫∑c t·ª´ gi·ªçng n√≥i (∆∞u ti√™n thanh chat)
        prompt = user_input or st.session_state.user_temp_input
        st.session_state.user_temp_input = ""  # Reset header c·ªßa microphone input

        # L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng v√† hi·ªÉn th·ªã
        st.session_state.messages.append({"role": "human", "content": prompt})
        st.chat_message("human").write(prompt)
        msgs.add_user_message(prompt)

        # X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi AI
        with st.chat_message("assistant"):
            st_callback = StreamlitCallbackHandler(st.container())
            
            # L·∫•y l·ªãch s·ª≠ h·ªôi tho·∫°i gi√† h∆°n
            chat_history = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]
            ]

            # ALGORITHM GPT processing
            response = agent_executor.invoke(
                {
                    "input": prompt,
                    "chat_history": chat_history
                },
                {"callbacks": [st_callback]}  # Callback ƒë·ªÉ stream output l√™n giao di·ªán
            )

            # Hi·ªÉn th·ªã ƒë√°p √°n c·ªßa AI
            output = response["output"]
            st.session_state.messages.append({"role": "assistant", "content": output})
            msgs.add_ai_message(output)
            st.write(output)

# === H√ÄM CH√çNH ===
def main():
    """
    H√†m ch√≠nh ƒëi·ªÅu khi·ªÉn lu·ªìng ch∆∞∆°ng tr√¨nh
    """
    initialize_app()
    model_choice, collection_to_query = setup_sidebar()
    msgs = setup_chat_interface(model_choice)
    
    # Kh·ªüi t·∫°o AI d·ª±a tr√™n l·ª±a ch·ªçn model ƒë·ªÉ tr·∫£ l·ªùi
    if model_choice == "OpenAI GPT-4":
        retriever = get_openai_retriever(collection_to_query)
        print(collection_to_query)
        agent_executor = get_openai_agent(retriever, "gpt4")
    elif model_choice == "OpenAI Grok":
        retriever = get_openai_retriever(collection_to_query)
        agent_executor = get_openai_agent(retriever, "grok")
    else:
        retriever = get_ollama_retriever(collection_to_query, use_ollama=True)
        print(collection_to_query)
        agent_executor = get_ollama_agent(retriever)

    handle_user_input_with_microphone(msgs, agent_executor)\
    


# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    main() 